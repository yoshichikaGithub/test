{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HisakaKoji/test/blob/master/chapter7_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJMSspVnVJ7p",
        "colab_type": "code",
        "outputId": "86cf175e-b278-4aca-f1bd-28367bc185ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-from-scratch.git\n",
        "!curl https://colab.chainer.org/install | sh -\n",
        "  \n",
        "  \n",
        "# coding: utf-8\n",
        "import cupy as np\n",
        "\n",
        "\n",
        "def smooth_curve(x):\n",
        "    \"\"\"損失関数のグラフを滑らかにするために用いる\n",
        "    参考：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
        "    \"\"\"\n",
        "    window_len = 11\n",
        "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
        "    w = np.kaiser(window_len, 2)\n",
        "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
        "    return y[5:len(y)-5]\n",
        "\n",
        "\n",
        "def shuffle_dataset(x, t):\n",
        "    \"\"\"データセットのシャッフルを行う\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 訓練データ\n",
        "    t : 教師データ\n",
        "    Returns\n",
        "    -------\n",
        "    x, t : シャッフルを行った訓練データと教師データ\n",
        "    \"\"\"\n",
        "    permutation = np.random.permutation(x.shape[0])\n",
        "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "    t = t[permutation]\n",
        "\n",
        "    return x, t\n",
        "\n",
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "    return (input_size + 2*pad - filter_size) / stride + 1\n",
        "\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
        "    filter_h : フィルターの高さ\n",
        "    filter_w : フィルターの幅\n",
        "    stride : ストライド\n",
        "    pad : パディング\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2次元配列\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    col :\n",
        "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
        "    filter_h :\n",
        "    filter_w\n",
        "    stride\n",
        "    pad\n",
        "    Returns\n",
        "    -------\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]\n",
        "#@title 7.4.3　Convolutionレイヤの実装\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append('deep-learning-from-scratch')  # 親ディレクトリのファイルをインポートするための設定\n",
        "#from common.util import im2col\n",
        "import cupy as np\n",
        "\n",
        "x1 = np.random.rand(1,3,7,7)\n",
        "col1 = im2col(x1,5,5,stride=1,pad=0)\n",
        "print(col1.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'deep-learning-from-scratch' already exists and is not an empty directory.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0  10748      0 --:--:-- --:--:-- --:--:-- 10821\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n",
            "(9, 75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LWlcYx1LCWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yuqq3eXVuC_",
        "colab_type": "code",
        "outputId": "e946d8e6-5f2f-4a9c-8df1-4bc3ed7f0561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x2 = np.random.rand(10,3,7,7)\n",
        "x2.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 3, 7, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3eSrRU7V9GN",
        "colab_type": "code",
        "outputId": "04e48208-0221-4de9-d6ad-9adc70929b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "col2 = im2col(x2,5,5,stride=1,pad=0)\n",
        "print(col2.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoHlctbRWd_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# coding: utf-8\n",
        "import cupy as np\n",
        "from common.functions import *\n",
        "#from common.util import im2col, col2im\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 中間データ（backward時に使用）\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 重み・バイアスパラメータの勾配\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWnB2e6oX1qB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title 7.5 CNNの実装\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append('deep-learning-from-scratch')  # 親ディレクトリのファイルをインポートするための設定\n",
        "import pickle\n",
        "import cupy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"単純なConvNet\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 入力サイズ（MNISTの場合は784）\n",
        "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
        "    output_size : 出力サイズ（MNISTの場合は10）\n",
        "    activation : 'relu' or 'sigmoid'\n",
        "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
        "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
        "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 重みの初期化\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"損失関数を求める\n",
        "        引数のxは入力データ、tは教師ラベル\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"勾配を求める（数値微分）\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 入力データ\n",
        "        t : 教師ラベル\n",
        "        Returns\n",
        "        -------\n",
        "        各層の勾配を持ったディクショナリ変数\n",
        "            grads['W1']、grads['W2']、...は各層の重み\n",
        "            grads['b1']、grads['b2']、...は各層のバイアス\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 入力データ\n",
        "        t : 教師ラベル\n",
        "        Returns\n",
        "        -------\n",
        "        各層の勾配を持ったディクショナリ変数\n",
        "            grads['W1']、grads['W2']、...は各層の重み\n",
        "            grads['b1']、grads['b2']、...は各層のバイアス\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YruPkKap6G0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "d212b9c5-9709-4ef0-b908-03926497e79f"
      },
      "source": [
        "#ランタイム⇒ランタイプのタイプを変更　⇒TPUを変更\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print ('TPU address is', tpu_address)\n",
        "\n",
        "with tf.Session(tpu_address) as session:\n",
        "  devices = session.list_devices()\n",
        "\n",
        "print ('TPU devices:')\n",
        "devices"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e2fd564b738b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtpu_address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grpc://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'TPU address is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB2pueJ7Yhwq",
        "colab_type": "code",
        "outputId": "3d8035d0-d8d4-4443-e9e6-21d3b4475c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "#10分ぐらいかかるのでスキップ推奨\n",
        "# coding: utf-8 \n",
        "import sys, os\n",
        "sys.path.append('./deep-learning-from-scratch/ch07')  # 親ディレクトリのファイルをインポートするための設定\n",
        "\n",
        "import cupy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 処理に時間のかかる場合はデータを削減 \n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# パラメータの保存\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# グラフの描画\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "train loss:2.299339846973684\n",
            "=== epoch:1, train acc:0.133, test acc:0.136 ===\n",
            "train loss:2.2977416345916795\n",
            "train loss:2.2914222237259185\n",
            "train loss:2.285467962198654\n",
            "train loss:2.2767152038382084\n",
            "train loss:2.2615907153701857\n",
            "train loss:2.252830865099954\n",
            "train loss:2.2112772818143416\n",
            "train loss:2.202058811768098\n",
            "train loss:2.2080106002880635\n",
            "train loss:2.1890473270618243\n",
            "train loss:2.113865699278488\n",
            "train loss:2.1003718493064345\n",
            "train loss:2.0277009214172046\n",
            "train loss:1.9693945724953252\n",
            "train loss:1.9112819481542602\n",
            "train loss:1.8674687417696672\n",
            "train loss:1.772996949886535\n",
            "train loss:1.7336412335566527\n",
            "train loss:1.6309385400923804\n",
            "train loss:1.5915204259522404\n",
            "train loss:1.4490761917575632\n",
            "train loss:1.3617770383244925\n",
            "train loss:1.307765544675814\n",
            "train loss:1.1622779944722337\n",
            "train loss:1.2337152482642169\n",
            "train loss:1.151090079014261\n",
            "train loss:1.1270195618402084\n",
            "train loss:0.9585752798092225\n",
            "train loss:0.8509753975444376\n",
            "train loss:0.8705172658317193\n",
            "train loss:0.885321341347911\n",
            "train loss:0.8916112173220629\n",
            "train loss:0.8249397417292811\n",
            "train loss:0.7188652115065992\n",
            "train loss:0.7601631822694063\n",
            "train loss:0.9951585856396563\n",
            "train loss:0.7201243341335196\n",
            "train loss:0.7345829198293563\n",
            "train loss:0.6573491354167932\n",
            "train loss:0.6715378498638072\n",
            "train loss:0.5998120639942403\n",
            "train loss:0.6699262945736443\n",
            "train loss:0.6022194884488018\n",
            "train loss:0.5059733977181201\n",
            "train loss:0.5983186086471496\n",
            "train loss:0.5724287846554816\n",
            "train loss:0.7091516216977856\n",
            "train loss:0.5235077445245567\n",
            "train loss:0.5912090370044683\n",
            "train loss:0.42890394160962036\n",
            "train loss:0.6343628714612294\n",
            "train loss:0.45070082535010064\n",
            "train loss:0.5992129872274634\n",
            "train loss:0.5537300935202893\n",
            "train loss:0.47593802617449293\n",
            "train loss:0.6459587077037459\n",
            "train loss:0.5173470570539875\n",
            "train loss:0.4569730571488742\n",
            "train loss:0.46686565975674277\n",
            "train loss:0.5495232822527547\n",
            "train loss:0.7761735320806247\n",
            "train loss:0.3936629149023767\n",
            "train loss:0.5806121126145669\n",
            "train loss:0.406848910781555\n",
            "train loss:0.38584087740012957\n",
            "train loss:0.423408744751328\n",
            "train loss:0.5852109050071266\n",
            "train loss:0.5379371604636387\n",
            "train loss:0.47695012175907464\n",
            "train loss:0.4906164581979806\n",
            "train loss:0.3410250954606844\n",
            "train loss:0.6565241876727628\n",
            "train loss:0.5331341784454157\n",
            "train loss:0.41685223760718465\n",
            "train loss:0.2861102447472644\n",
            "train loss:0.58651549416696\n",
            "train loss:0.5551921903097468\n",
            "train loss:0.4479958690977463\n",
            "train loss:0.4338951301980209\n",
            "train loss:0.48018035183092406\n",
            "train loss:0.396165411384486\n",
            "train loss:0.3131471629379272\n",
            "train loss:0.5450834109422743\n",
            "train loss:0.23167457885204037\n",
            "train loss:0.47919879975445445\n",
            "train loss:0.47914829423224986\n",
            "train loss:0.584655714354326\n",
            "train loss:0.5000341768731613\n",
            "train loss:0.3604906174209319\n",
            "train loss:0.2961869582575006\n",
            "train loss:0.36316878116530255\n",
            "train loss:0.37380633353649206\n",
            "train loss:0.21848571764198874\n",
            "train loss:0.30686993065098384\n",
            "train loss:0.19858660489712565\n",
            "train loss:0.3878268834925925\n",
            "train loss:0.24216107760819036\n",
            "train loss:0.4769530464157202\n",
            "train loss:0.7356415552984866\n",
            "train loss:0.3839894564565901\n",
            "train loss:0.4259253685043147\n",
            "train loss:0.30643612632412337\n",
            "train loss:0.4003348134510756\n",
            "train loss:0.41003567781083966\n",
            "train loss:0.35281905039840333\n",
            "train loss:0.29031534599477554\n",
            "train loss:0.47477180839001887\n",
            "train loss:0.37646834310760247\n",
            "train loss:0.30028928015990397\n",
            "train loss:0.4256802048469023\n",
            "train loss:0.42710323029750513\n",
            "train loss:0.40759932859371817\n",
            "train loss:0.4210102303834455\n",
            "train loss:0.30298551309592886\n",
            "train loss:0.5306290868552292\n",
            "train loss:0.2971733909803659\n",
            "train loss:0.2634839437977215\n",
            "train loss:0.2939800876336436\n",
            "train loss:0.467682909068557\n",
            "train loss:0.3535644372178745\n",
            "train loss:0.35435851186761363\n",
            "train loss:0.2883387036940312\n",
            "train loss:0.28258532011019905\n",
            "train loss:0.41033944126238714\n",
            "train loss:0.28093434645015636\n",
            "train loss:0.29158409704002325\n",
            "train loss:0.3271041601338834\n",
            "train loss:0.3766022532989778\n",
            "train loss:0.25454319292857913\n",
            "train loss:0.26894985283211126\n",
            "train loss:0.23962478429353185\n",
            "train loss:0.41484126562187545\n",
            "train loss:0.3610676079724486\n",
            "train loss:0.29616382462454455\n",
            "train loss:0.3469831638396286\n",
            "train loss:0.3204225331874016\n",
            "train loss:0.4337549400604268\n",
            "train loss:0.4781176496113565\n",
            "train loss:0.3755496055812257\n",
            "train loss:0.3183626540884431\n",
            "train loss:0.5488377216085474\n",
            "train loss:0.46483634922766415\n",
            "train loss:0.3443658825205114\n",
            "train loss:0.2945633648869338\n",
            "train loss:0.43564954730576133\n",
            "train loss:0.1882265131681792\n",
            "train loss:0.22616081116963915\n",
            "train loss:0.25004985490667697\n",
            "train loss:0.4910227565793845\n",
            "train loss:0.33779926381575104\n",
            "train loss:0.27112347342224996\n",
            "train loss:0.18013090565106926\n",
            "train loss:0.24825862703856885\n",
            "train loss:0.3290754620670612\n",
            "train loss:0.33528744286319123\n",
            "train loss:0.34578619776481084\n",
            "train loss:0.34802376658669926\n",
            "train loss:0.4006184201118782\n",
            "train loss:0.3274509819360887\n",
            "train loss:0.36416708002631415\n",
            "train loss:0.20758487475841206\n",
            "train loss:0.2876161134714297\n",
            "train loss:0.18798995162665924\n",
            "train loss:0.2335573704189667\n",
            "train loss:0.19415091342170662\n",
            "train loss:0.35629789887946073\n",
            "train loss:0.32435743714969645\n",
            "train loss:0.23256401994647782\n",
            "train loss:0.3431288749805622\n",
            "train loss:0.3538403668093491\n",
            "train loss:0.4460444801177947\n",
            "train loss:0.29293090597023047\n",
            "train loss:0.29241841542405456\n",
            "train loss:0.3608449600422351\n",
            "train loss:0.15255385085349377\n",
            "train loss:0.23811988813524068\n",
            "train loss:0.42027730989243467\n",
            "train loss:0.3853705361466747\n",
            "train loss:0.2281432470810506\n",
            "train loss:0.2859315581773705\n",
            "train loss:0.3998497683530464\n",
            "train loss:0.28195736816345796\n",
            "train loss:0.3432519088620083\n",
            "train loss:0.376242063664822\n",
            "train loss:0.3853517587609892\n",
            "train loss:0.241874947872426\n",
            "train loss:0.3075473796059958\n",
            "train loss:0.4474214595949787\n",
            "train loss:0.27847689364860667\n",
            "train loss:0.14287886909707678\n",
            "train loss:0.3475394364119621\n",
            "train loss:0.2932005545471379\n",
            "train loss:0.37108753525869154\n",
            "train loss:0.35769981375872123\n",
            "train loss:0.34506591994638636\n",
            "train loss:0.19974915478497987\n",
            "train loss:0.13114700894792272\n",
            "train loss:0.17114602157557696\n",
            "train loss:0.2284919805031654\n",
            "train loss:0.3586202565515979\n",
            "train loss:0.3550665488851928\n",
            "train loss:0.24402177010570564\n",
            "train loss:0.2044588725245843\n",
            "train loss:0.28018143260286227\n",
            "train loss:0.2741635038261873\n",
            "train loss:0.5692083752589175\n",
            "train loss:0.4013724944280359\n",
            "train loss:0.20624315778496935\n",
            "train loss:0.247925993106768\n",
            "train loss:0.35666567723569254\n",
            "train loss:0.45542552620579985\n",
            "train loss:0.35385648656711055\n",
            "train loss:0.3344690825337137\n",
            "train loss:0.18177991161372467\n",
            "train loss:0.37394183851836355\n",
            "train loss:0.25956448326893894\n",
            "train loss:0.332858735600823\n",
            "train loss:0.28215921149141887\n",
            "train loss:0.19634190884899347\n",
            "train loss:0.21571035531422364\n",
            "train loss:0.28362028415536045\n",
            "train loss:0.21975179121222563\n",
            "train loss:0.43584502748434095\n",
            "train loss:0.14306363488576068\n",
            "train loss:0.2268256616366399\n",
            "train loss:0.2721982488617848\n",
            "train loss:0.1860364646721478\n",
            "train loss:0.31727911383234053\n",
            "train loss:0.2298669993495478\n",
            "train loss:0.24892557193998047\n",
            "train loss:0.1799096254274638\n",
            "train loss:0.14753054722469547\n",
            "train loss:0.19834664419544507\n",
            "train loss:0.22768355349645394\n",
            "train loss:0.2759814034773405\n",
            "train loss:0.3774375618326726\n",
            "train loss:0.3647926379220771\n",
            "train loss:0.2227293676465218\n",
            "train loss:0.1394588668123483\n",
            "train loss:0.296764659229046\n",
            "train loss:0.2564161475129019\n",
            "train loss:0.20305368698297963\n",
            "train loss:0.21918153001271146\n",
            "train loss:0.1822840545623007\n",
            "train loss:0.16231159045182694\n",
            "train loss:0.30604329198589464\n",
            "train loss:0.1675670304757871\n",
            "train loss:0.25337699696576627\n",
            "train loss:0.2601295651481653\n",
            "train loss:0.25919315499619755\n",
            "train loss:0.2967745155376914\n",
            "train loss:0.34480280213767045\n",
            "train loss:0.2644859822354036\n",
            "train loss:0.33672725377292273\n",
            "train loss:0.3022823976308526\n",
            "train loss:0.19344387944396554\n",
            "train loss:0.18981195394513306\n",
            "train loss:0.33567255600187573\n",
            "train loss:0.27889172602713364\n",
            "train loss:0.24859296009849077\n",
            "train loss:0.31788423641696\n",
            "train loss:0.2599232664462223\n",
            "train loss:0.25592775576047844\n",
            "train loss:0.23922128236120824\n",
            "train loss:0.23441873681273825\n",
            "train loss:0.21975051917204222\n",
            "train loss:0.28616556515216396\n",
            "train loss:0.14992337990563864\n",
            "train loss:0.13125055798062712\n",
            "train loss:0.24383142371932312\n",
            "train loss:0.27153189751701795\n",
            "train loss:0.2507219233639026\n",
            "train loss:0.23068852952523577\n",
            "train loss:0.21715591619492983\n",
            "train loss:0.25581408814671985\n",
            "train loss:0.19944493601481397\n",
            "train loss:0.36602666246390325\n",
            "train loss:0.3439214668010921\n",
            "train loss:0.19702139549292358\n",
            "train loss:0.20976704090448683\n",
            "train loss:0.21778728181476303\n",
            "train loss:0.21200860360140034\n",
            "train loss:0.2840688808691447\n",
            "train loss:0.16935331738393075\n",
            "train loss:0.33202247752133507\n",
            "train loss:0.09762964783479988\n",
            "train loss:0.320326528208567\n",
            "train loss:0.24610450277559484\n",
            "train loss:0.21273643528749733\n",
            "train loss:0.22729394257492924\n",
            "train loss:0.38239074054490063\n",
            "train loss:0.3120460587780891\n",
            "train loss:0.2855245754358927\n",
            "train loss:0.21993678296771135\n",
            "train loss:0.2620400204760399\n",
            "train loss:0.13439561039363984\n",
            "train loss:0.19189258570322312\n",
            "train loss:0.2470043039738259\n",
            "train loss:0.24955801593291016\n",
            "train loss:0.26134385756264955\n",
            "train loss:0.17563292536439243\n",
            "train loss:0.2058473719446437\n",
            "train loss:0.20065376906282065\n",
            "train loss:0.143971045863902\n",
            "train loss:0.12017323906431698\n",
            "train loss:0.1393642812111856\n",
            "train loss:0.2823392446181598\n",
            "train loss:0.18056018638490814\n",
            "train loss:0.19794527956525965\n",
            "train loss:0.14028462569220004\n",
            "train loss:0.236448679126348\n",
            "train loss:0.19394647554604938\n",
            "train loss:0.249806814135696\n",
            "train loss:0.12008245447137376\n",
            "train loss:0.18871749871751242\n",
            "train loss:0.09950587703080678\n",
            "train loss:0.3570630721803487\n",
            "train loss:0.31149843104129693\n",
            "train loss:0.12377203853883084\n",
            "train loss:0.22057932976312325\n",
            "train loss:0.21328276004320962\n",
            "train loss:0.29312104788002286\n",
            "train loss:0.26632570314713766\n",
            "train loss:0.21986559578941434\n",
            "train loss:0.1621623055702099\n",
            "train loss:0.13751030059063024\n",
            "train loss:0.1654122386146158\n",
            "train loss:0.1100215245687246\n",
            "train loss:0.25675858976726107\n",
            "train loss:0.13848014528012295\n",
            "train loss:0.2518287355821119\n",
            "train loss:0.11439938824270893\n",
            "train loss:0.16227375902438246\n",
            "train loss:0.1782701458918417\n",
            "train loss:0.14631310509917475\n",
            "train loss:0.2853145052177833\n",
            "train loss:0.13966107034530664\n",
            "train loss:0.2438570244300478\n",
            "train loss:0.17408165983454263\n",
            "train loss:0.16101703071024528\n",
            "train loss:0.11698142228938259\n",
            "train loss:0.12822791155379756\n",
            "train loss:0.11951736006713988\n",
            "train loss:0.30297426125911914\n",
            "train loss:0.18050613179975422\n",
            "train loss:0.21475941385316744\n",
            "train loss:0.18661888372881857\n",
            "train loss:0.1935950595280776\n",
            "train loss:0.13727539197868435\n",
            "train loss:0.14240310252326985\n",
            "train loss:0.3411621428599782\n",
            "train loss:0.20884640879837643\n",
            "train loss:0.12322097251245336\n",
            "train loss:0.1864719392924298\n",
            "train loss:0.18563247078584863\n",
            "train loss:0.12248796522173834\n",
            "train loss:0.22773971393555315\n",
            "train loss:0.25677615555699074\n",
            "train loss:0.12245483372439635\n",
            "train loss:0.12901739628352746\n",
            "train loss:0.2918608395643951\n",
            "train loss:0.1818094809709697\n",
            "train loss:0.30112696062097283\n",
            "train loss:0.2012483801854289\n",
            "train loss:0.18730570739674784\n",
            "train loss:0.16548026599071064\n",
            "train loss:0.17425244290546574\n",
            "train loss:0.23589758707944675\n",
            "train loss:0.12963701311515588\n",
            "train loss:0.15944513018413536\n",
            "train loss:0.19594017846920012\n",
            "train loss:0.16602640745228303\n",
            "train loss:0.19340698796854666\n",
            "train loss:0.15613632805192204\n",
            "train loss:0.21471248199448179\n",
            "train loss:0.1977387775118113\n",
            "train loss:0.10332289123044142\n",
            "train loss:0.05275678334919564\n",
            "train loss:0.13628824998233902\n",
            "train loss:0.10047649498512934\n",
            "train loss:0.11785662619606255\n",
            "train loss:0.19507268254949878\n",
            "train loss:0.17922127472297034\n",
            "train loss:0.160969669977435\n",
            "train loss:0.16257863759070731\n",
            "train loss:0.15019992454609057\n",
            "train loss:0.17826328382751483\n",
            "train loss:0.16696771431976615\n",
            "train loss:0.15797973411981694\n",
            "train loss:0.12296932020688214\n",
            "train loss:0.16709024155896857\n",
            "train loss:0.11591294209492167\n",
            "train loss:0.1442982208716\n",
            "train loss:0.1417246933938637\n",
            "train loss:0.1625260682104864\n",
            "train loss:0.27807704446657344\n",
            "train loss:0.1081853567092653\n",
            "train loss:0.1128271988140705\n",
            "train loss:0.09037579127059077\n",
            "train loss:0.12716056595297343\n",
            "train loss:0.21817441493773096\n",
            "train loss:0.2688258423949595\n",
            "train loss:0.2196528084172254\n",
            "train loss:0.14865597486571505\n",
            "train loss:0.1563867027487215\n",
            "train loss:0.11223396663155101\n",
            "train loss:0.14785718880377877\n",
            "train loss:0.108285749765883\n",
            "train loss:0.2738210989444955\n",
            "train loss:0.1795255425877485\n",
            "train loss:0.16058850271082684\n",
            "train loss:0.24261856526918216\n",
            "train loss:0.1620323876013318\n",
            "train loss:0.21107903733184086\n",
            "train loss:0.1509032260316097\n",
            "train loss:0.2035718845637448\n",
            "train loss:0.1573380556553298\n",
            "train loss:0.13957513656640605\n",
            "train loss:0.0683985379821362\n",
            "train loss:0.1837745632825546\n",
            "train loss:0.22363582913690827\n",
            "train loss:0.1363240151573628\n",
            "train loss:0.19957455088809462\n",
            "train loss:0.13365709104196938\n",
            "train loss:0.20574486537704637\n",
            "train loss:0.2164469308682163\n",
            "train loss:0.12765089358335063\n",
            "train loss:0.1227381218195959\n",
            "train loss:0.11560312368927242\n",
            "train loss:0.25059528052795604\n",
            "train loss:0.21145232038006895\n",
            "train loss:0.1376951898784419\n",
            "train loss:0.2469112578001046\n",
            "train loss:0.1794924823134028\n",
            "train loss:0.10294766748926065\n",
            "train loss:0.19277323561226695\n",
            "train loss:0.12876659491832643\n",
            "train loss:0.13084685020469777\n",
            "train loss:0.19365662447698154\n",
            "train loss:0.12971343530385904\n",
            "train loss:0.1500454332435353\n",
            "train loss:0.1522795038161865\n",
            "train loss:0.21844344672229613\n",
            "train loss:0.1654381833883022\n",
            "train loss:0.22344677208078503\n",
            "train loss:0.07841154271046345\n",
            "train loss:0.1669520147178509\n",
            "train loss:0.14754655953563922\n",
            "train loss:0.10442662441468901\n",
            "train loss:0.16400753026940598\n",
            "train loss:0.1944067702052873\n",
            "train loss:0.23143662470245963\n",
            "train loss:0.10333895491525107\n",
            "train loss:0.31553505200929516\n",
            "train loss:0.1692802429516805\n",
            "train loss:0.09919919823079494\n",
            "train loss:0.23555132986165433\n",
            "train loss:0.1059855100146493\n",
            "train loss:0.07951176586203419\n",
            "train loss:0.18075322947436212\n",
            "train loss:0.1667083689335407\n",
            "train loss:0.19464500788603953\n",
            "train loss:0.1938807405322053\n",
            "train loss:0.04695671359187685\n",
            "train loss:0.16062884595945393\n",
            "train loss:0.14242053997153412\n",
            "train loss:0.07544812069552584\n",
            "train loss:0.1830323375857093\n",
            "train loss:0.12316176390510919\n",
            "train loss:0.1719736657148401\n",
            "train loss:0.08287346763487745\n",
            "train loss:0.16386474374247834\n",
            "train loss:0.15526470360211822\n",
            "train loss:0.24000852020401692\n",
            "train loss:0.12282147648726653\n",
            "train loss:0.10206652598475106\n",
            "train loss:0.1389161718911035\n",
            "train loss:0.12541346581520085\n",
            "train loss:0.23492711817801126\n",
            "train loss:0.12683127700626845\n",
            "train loss:0.1474846057656123\n",
            "train loss:0.21182216434159673\n",
            "train loss:0.17624765059264752\n",
            "train loss:0.2643804087174127\n",
            "train loss:0.09738717856068461\n",
            "train loss:0.07460722689648298\n",
            "train loss:0.12066743841655964\n",
            "train loss:0.24485783644738293\n",
            "train loss:0.19616714999597076\n",
            "train loss:0.08583169517367514\n",
            "train loss:0.17696016946231427\n",
            "train loss:0.32238574385925206\n",
            "train loss:0.17486147978025512\n",
            "train loss:0.09100489169958687\n",
            "train loss:0.15017047918675805\n",
            "train loss:0.09214638451890965\n",
            "train loss:0.20726710860547778\n",
            "train loss:0.1697057703613352\n",
            "train loss:0.0662019008651238\n",
            "train loss:0.08284319149008867\n",
            "train loss:0.1000089102619592\n",
            "train loss:0.18903172999042767\n",
            "train loss:0.19725636568260055\n",
            "train loss:0.2321089881429835\n",
            "train loss:0.2006002600762714\n",
            "train loss:0.10154597875788637\n",
            "train loss:0.26921585970590434\n",
            "train loss:0.1460437670726668\n",
            "train loss:0.19620271443886966\n",
            "train loss:0.059659800644598394\n",
            "train loss:0.09027764761393328\n",
            "train loss:0.10365832929128109\n",
            "train loss:0.1480094748729574\n",
            "train loss:0.18340382409134456\n",
            "train loss:0.11599716220904269\n",
            "train loss:0.27537746568221527\n",
            "train loss:0.22938095512659148\n",
            "train loss:0.14654749795577587\n",
            "train loss:0.08828352117099829\n",
            "train loss:0.17255902174949714\n",
            "train loss:0.1610099394461777\n",
            "train loss:0.09394725201006046\n",
            "train loss:0.11239675420587503\n",
            "train loss:0.1093893277236814\n",
            "train loss:0.12225177467970601\n",
            "train loss:0.20415698284093584\n",
            "train loss:0.21234971281976922\n",
            "train loss:0.10036233377413188\n",
            "train loss:0.09329292711546583\n",
            "train loss:0.08083209397238898\n",
            "train loss:0.10305673922318725\n",
            "train loss:0.15499074574922425\n",
            "train loss:0.10117608536610753\n",
            "train loss:0.1793415474322374\n",
            "train loss:0.12542926391226017\n",
            "train loss:0.12869882748925698\n",
            "train loss:0.07259890919408496\n",
            "train loss:0.15478235434335086\n",
            "train loss:0.11349105940315334\n",
            "train loss:0.10812219450851593\n",
            "train loss:0.07007446841852424\n",
            "train loss:0.08380872581712985\n",
            "train loss:0.2500186606855073\n",
            "train loss:0.13715514460104541\n",
            "train loss:0.06492345405204714\n",
            "train loss:0.19130113905754964\n",
            "train loss:0.18802708364227375\n",
            "train loss:0.12686121120062743\n",
            "train loss:0.1914201913945857\n",
            "train loss:0.1976717943297122\n",
            "train loss:0.09558170327860559\n",
            "train loss:0.12608882060927323\n",
            "train loss:0.10823824591388831\n",
            "train loss:0.22453856272595282\n",
            "train loss:0.08327306818194673\n",
            "train loss:0.07119278386511545\n",
            "train loss:0.09263716393448936\n",
            "train loss:0.12307680435412076\n",
            "train loss:0.15819271749800326\n",
            "train loss:0.136204400253273\n",
            "train loss:0.17455895987892142\n",
            "train loss:0.12236764069620239\n",
            "train loss:0.09443085707665964\n",
            "train loss:0.13840904416094515\n",
            "train loss:0.05854105744238183\n",
            "train loss:0.16203956960692248\n",
            "train loss:0.12757914496994804\n",
            "train loss:0.06725178663968745\n",
            "train loss:0.10571195276302658\n",
            "train loss:0.14917749660369295\n",
            "train loss:0.12332857032216811\n",
            "train loss:0.1441974820442742\n",
            "train loss:0.10300811403129868\n",
            "train loss:0.06402488828360699\n",
            "train loss:0.08641770053179101\n",
            "train loss:0.23523298370676005\n",
            "train loss:0.1305379106226822\n",
            "train loss:0.09931728082544469\n",
            "train loss:0.11661347679545803\n",
            "train loss:0.05236736290142559\n",
            "train loss:0.1309543782471343\n",
            "train loss:0.25963050947290456\n",
            "train loss:0.14533551514731669\n",
            "train loss:0.15524676775191415\n",
            "train loss:0.09830635165142919\n",
            "train loss:0.19916496553398758\n",
            "train loss:0.03788789772853243\n",
            "train loss:0.14340917724728444\n",
            "train loss:0.06769823492775336\n",
            "train loss:0.18685247553357917\n",
            "train loss:0.14034761336528773\n",
            "train loss:0.14771240083099613\n",
            "train loss:0.10101274166765412\n",
            "train loss:0.11112973342656833\n",
            "train loss:0.11063244750968386\n",
            "train loss:0.03891524995758215\n",
            "train loss:0.05699740938357189\n",
            "train loss:0.29145797219215963\n",
            "train loss:0.08267858320879025\n",
            "train loss:0.1373003500827979\n",
            "=== epoch:2, train acc:0.964, test acc:0.969 ===\n",
            "train loss:0.09411573432449022\n",
            "train loss:0.04685914541228744\n",
            "train loss:0.03931139771170055\n",
            "train loss:0.10372683418412304\n",
            "train loss:0.09831625706216482\n",
            "train loss:0.17083759971311668\n",
            "train loss:0.07884540075010976\n",
            "train loss:0.235252394120005\n",
            "train loss:0.0865511245500797\n",
            "train loss:0.07421305247203526\n",
            "train loss:0.09418857013962866\n",
            "train loss:0.10642525490395721\n",
            "train loss:0.17529903535035188\n",
            "train loss:0.06947775737817785\n",
            "train loss:0.11327892256694896\n",
            "train loss:0.11620109034888118\n",
            "train loss:0.06970367825875388\n",
            "train loss:0.09599815751541364\n",
            "train loss:0.036289064230732734\n",
            "train loss:0.12436285398987847\n",
            "train loss:0.2563859658063236\n",
            "train loss:0.04540289037821671\n",
            "train loss:0.1906129345249475\n",
            "train loss:0.19107752271722137\n",
            "train loss:0.06971983637851163\n",
            "train loss:0.08440952775932505\n",
            "train loss:0.04569160870102739\n",
            "train loss:0.0795083073589637\n",
            "train loss:0.08536931893720576\n",
            "train loss:0.07116230286265714\n",
            "train loss:0.15897187595175907\n",
            "train loss:0.06892154925906911\n",
            "train loss:0.24591733971152593\n",
            "train loss:0.10814870582220051\n",
            "train loss:0.13129235978717158\n",
            "train loss:0.15771995598757083\n",
            "train loss:0.034228131904314174\n",
            "train loss:0.12096440527667146\n",
            "train loss:0.1335795448019695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYB5dGaMZRxU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "a5d65b44-63bd-4d40-9919-785b1be0f574"
      },
      "source": [
        "#@title CNNの可視化\n",
        "sys.path.append('deep-learning-from-scratch/ch07') \n",
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from simple_convnet import SimpleConvNet\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# ランダム初期化後の重み\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 学習後の重み\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/hdN+Cf/zMq4QAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQAA578AW92Xa5OMEr0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}