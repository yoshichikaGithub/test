{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HisakaKoji/test/blob/master/chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJMSspVnVJ7p",
        "colab_type": "code",
        "outputId": "0f09ff2d-e9f9-4988-b37c-860fd82862a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title 7.4.3　Convolutionレイヤの実装\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append('deep-learning-from-scratch')  # 親ディレクトリのファイルをインポートするための設定\n",
        "from common.util import im2col\n",
        "import numpy as np\n",
        "\n",
        "x1 = np.random.rand(1,3,7,7)\n",
        "col1 = im2col(x1,5,5,stride=1,pad=0)\n",
        "print(col1.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9, 75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yuqq3eXVuC_",
        "colab_type": "code",
        "outputId": "4f9ae0ff-9e7e-491a-b286-426b2f8341e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x2 = np.random.rand(10,3,7,7)\n",
        "x2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 3, 7, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3eSrRU7V9GN",
        "colab_type": "code",
        "outputId": "93662f8f-97bd-482b-fa9e-ae291d3f4222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "col2 = im2col(x2,5,5,stride=1,pad=0)\n",
        "print(col2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoHlctbRWd_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# coding: utf-8\n",
        "import numpy as np\n",
        "from common.functions import *\n",
        "from common.util import im2col, col2im\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 中間データ（backward時に使用）\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 重み・バイアスパラメータの勾配\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWnB2e6oX1qB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title 7.5 CNNの実装\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append('deep-learning-from-scratch')  # 親ディレクトリのファイルをインポートするための設定\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"単純なConvNet\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 入力サイズ（MNISTの場合は784）\n",
        "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
        "    output_size : 出力サイズ（MNISTの場合は10）\n",
        "    activation : 'relu' or 'sigmoid'\n",
        "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
        "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
        "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 重みの初期化\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"損失関数を求める\n",
        "        引数のxは入力データ、tは教師ラベル\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"勾配を求める（数値微分）\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 入力データ\n",
        "        t : 教師ラベル\n",
        "        Returns\n",
        "        -------\n",
        "        各層の勾配を持ったディクショナリ変数\n",
        "            grads['W1']、grads['W2']、...は各層の重み\n",
        "            grads['b1']、grads['b2']、...は各層のバイアス\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 入力データ\n",
        "        t : 教師ラベル\n",
        "        Returns\n",
        "        -------\n",
        "        各層の勾配を持ったディクショナリ変数\n",
        "            grads['W1']、grads['W2']、...は各層の重み\n",
        "            grads['b1']、grads['b2']、...は各層のバイアス\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB2pueJ7Yhwq",
        "colab_type": "code",
        "outputId": "51db88a6-a8f0-40de-f651-c2611147a856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#10分ぐらいかかるのでスキップ推奨\n",
        "# coding: utf-8 \n",
        "import sys, os\n",
        "sys.path.append('deep-learning-from-scratch/ch07')  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 処理に時間のかかる場合はデータを削減 \n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# パラメータの保存\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# グラフの描画\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.2999708763024187\n",
            "=== epoch:1, train acc:0.177, test acc:0.192 ===\n",
            "train loss:2.2976452723656515\n",
            "train loss:2.293574936077937\n",
            "train loss:2.2898487595610693\n",
            "train loss:2.282068345560931\n",
            "train loss:2.2723944132818987\n",
            "train loss:2.2542946188602038\n",
            "train loss:2.238638905474455\n",
            "train loss:2.2258669375356828\n",
            "train loss:2.198324175179337\n",
            "train loss:2.1719355959388658\n",
            "train loss:2.1333848755118767\n",
            "train loss:2.1057114924558884\n",
            "train loss:2.0530385229831656\n",
            "train loss:1.9897761591121323\n",
            "train loss:1.9297874473974088\n",
            "train loss:1.8189378106534508\n",
            "train loss:1.7846934905185048\n",
            "train loss:1.752299478832233\n",
            "train loss:1.6817677785829084\n",
            "train loss:1.5419416213725359\n",
            "train loss:1.4568051587812407\n",
            "train loss:1.4151813033520382\n",
            "train loss:1.255845708695545\n",
            "train loss:1.2468478850978746\n",
            "train loss:1.1947409499214858\n",
            "train loss:1.0533104472296317\n",
            "train loss:1.0528603881859666\n",
            "train loss:1.0103082767049176\n",
            "train loss:0.834495749961917\n",
            "train loss:0.8802372668401678\n",
            "train loss:0.9179279385362209\n",
            "train loss:0.8208098393742925\n",
            "train loss:0.7909297153050169\n",
            "train loss:0.8160489756499075\n",
            "train loss:0.5915802584488515\n",
            "train loss:0.6945702131583968\n",
            "train loss:0.700845279284603\n",
            "train loss:0.6961283600775642\n",
            "train loss:0.5581164326177278\n",
            "train loss:0.4750361024332948\n",
            "train loss:0.569352759181248\n",
            "train loss:0.7159923185501165\n",
            "train loss:0.5623847682554607\n",
            "train loss:0.5520169246279543\n",
            "train loss:0.5794855867435303\n",
            "train loss:0.5153785996169539\n",
            "train loss:0.5615163614821067\n",
            "train loss:0.5975993195896134\n",
            "train loss:0.5444594539938561\n",
            "train loss:0.49024434772421316\n",
            "train loss:0.641028903401964\n",
            "train loss:0.4033113597021814\n",
            "train loss:0.3448680877532189\n",
            "train loss:0.4407282591080703\n",
            "train loss:0.49631531294296244\n",
            "train loss:0.3866114131599325\n",
            "train loss:0.3865031387215977\n",
            "train loss:0.5681032531270922\n",
            "train loss:0.5233624305530666\n",
            "train loss:0.44821955565103044\n",
            "train loss:0.6851177952495219\n",
            "train loss:0.4712196780168864\n",
            "train loss:0.5031486923562757\n",
            "train loss:0.5684454482753042\n",
            "train loss:0.6430273887144438\n",
            "train loss:0.5551949452800562\n",
            "train loss:0.5163933822306237\n",
            "train loss:0.4297357533377894\n",
            "train loss:0.30150857762623373\n",
            "train loss:0.36465606149069474\n",
            "train loss:0.3898098391830874\n",
            "train loss:0.4899661081556368\n",
            "train loss:0.42270778284841215\n",
            "train loss:0.4597946047765263\n",
            "train loss:0.45924317760767736\n",
            "train loss:0.37081620389098807\n",
            "train loss:0.37317507322359383\n",
            "train loss:0.34001748671757254\n",
            "train loss:0.2833999363507098\n",
            "train loss:0.4921623103089082\n",
            "train loss:0.35905571348057885\n",
            "train loss:0.351785450552414\n",
            "train loss:0.36793928139609455\n",
            "train loss:0.4285694203818191\n",
            "train loss:0.32054017516218786\n",
            "train loss:0.35708341788582953\n",
            "train loss:0.3491167763520097\n",
            "train loss:0.306346138143495\n",
            "train loss:0.2724357681805652\n",
            "train loss:0.4204781577033014\n",
            "train loss:0.36594440536927464\n",
            "train loss:0.6510096171331285\n",
            "train loss:0.29074827814511356\n",
            "train loss:0.3892884232439276\n",
            "train loss:0.6339172724041816\n",
            "train loss:0.3211988659174379\n",
            "train loss:0.43962604490542745\n",
            "train loss:0.3335267612044748\n",
            "train loss:0.4779826072166032\n",
            "train loss:0.3196443579694803\n",
            "train loss:0.33112872891911144\n",
            "train loss:0.35236483485721465\n",
            "train loss:0.20207204152836655\n",
            "train loss:0.44966478045309033\n",
            "train loss:0.3223661344924205\n",
            "train loss:0.4773669580382476\n",
            "train loss:0.35845716324588855\n",
            "train loss:0.35885221693135827\n",
            "train loss:0.384735478655325\n",
            "train loss:0.5550233340138946\n",
            "train loss:0.3612779367931775\n",
            "train loss:0.4210824967835533\n",
            "train loss:0.3341105409611066\n",
            "train loss:0.5627477620410105\n",
            "train loss:0.39808795350266585\n",
            "train loss:0.2408746438052697\n",
            "train loss:0.42232966688837303\n",
            "train loss:0.2606067584312571\n",
            "train loss:0.23610776658362068\n",
            "train loss:0.3148799822336736\n",
            "train loss:0.27852110158968857\n",
            "train loss:0.4678386421249466\n",
            "train loss:0.24957898218860822\n",
            "train loss:0.2039551196347023\n",
            "train loss:0.2928076516954787\n",
            "train loss:0.40628159466789\n",
            "train loss:0.2890698278843686\n",
            "train loss:0.2704652277009815\n",
            "train loss:0.2367088854915752\n",
            "train loss:0.3674924210208974\n",
            "train loss:0.3509640678343373\n",
            "train loss:0.2645941811169934\n",
            "train loss:0.32356275994753786\n",
            "train loss:0.3330683735926898\n",
            "train loss:0.2658403838038166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYB5dGaMZRxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title CNNの可視化\n",
        "sys.path.append('deep-learning-from-scratch/ch07') \n",
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from simple_convnet import SimpleConvNet\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# ランダム初期化後の重み\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 学習後の重み\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}